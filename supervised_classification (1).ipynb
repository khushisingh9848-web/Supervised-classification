{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ques-1>What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Ans>Information Gain (IG)-> is a metric used in Decision Trees to measure how well a feature separates the training data into target classes.\n",
        "It tells us how much uncertainty (impurity) is reduced after splitting the dataset on a particular feature.\n",
        "\n",
        "Information Gain Formula\n",
        "IG(S,A)=Entropy(S)−∑(∣S∣∣Sv​∣​×Entropy(Sv​))\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "\n",
        "S = original dataset\n",
        "\n",
        "\n",
        "A = attribute (feature)\n",
        "\n",
        "Sv= subset after split\n",
        "\n",
        "\n",
        "How Information Gain is Used in Decision TreesL:\n",
        "\n",
        "1.Calculate the entropy of the full dataset\n",
        "\n",
        "2.For each feature:\n",
        "\n",
        "   Split the data based on feature values\n",
        "\n",
        "  Calculate entropy for each split\n",
        "\n",
        "3.Compute Information Gain for each feature\n",
        "\n",
        "4.Select the feature with maximum Information Gain as the splitting node\n",
        "\n",
        "5.Repeat the process recursively for child nodes\n"
      ],
      "metadata": {
        "id": "Jr37ozHagoKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-2> What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Ans>Gini Impurity:\n",
        "Gini Impurity measures how often a randomly chosen data point would be incorrectly classified if it were randomly labeled according to the class distribution.\n",
        "\n",
        " Formula>  \n",
        " Gini=1−∑pi2​\n",
        "\n",
        "Entropy:\n",
        "Entropy measures the amount of uncertainty or randomness in the dataset.\n",
        "\n",
        "Entropy=−∑pi​log2​(pi​)\n",
        "\n",
        "\n",
        "Difference between Gini Impurity and Entropy:\n",
        "\n",
        "| Aspect      | Gini Impurity                          | Entropy                            |\n",
        "| ----------- | -------------------------------------- | ---------------------------------- |\n",
        "| Concept     | Measures misclassification probability | Measures randomness or uncertainty |\n",
        "| Range       | 0 to 0.5 (binary classification)       | 0 to 1 (binary classification)     |\n",
        "| Best Value  | 0 (pure node)                          | 0 (pure node)                      |\n",
        "| Worst Case  | 0.5 (equal class split)                | 1 (equal class split)              |\n",
        "| Computation | Simpler and faster                     | Slightly more complex (logarithms) |\n",
        "| Used In     | CART Decision Trees                    | ID3, C4.5 Decision Trees           |\n",
        "| Sensitivity | Less sensitive to class changes        | More sensitive to small changes    |\n"
      ],
      "metadata": {
        "id": "uxQJ_FTYgn93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 3>What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Ans>Pre-Pruning is a technique used in Decision Trees where the growth of the tree is stopped early before it becomes too complex.\n",
        "The tree is not allowed to fully grow if further splitting does not significantly improve performance.\n",
        "\n",
        "Purpose of Pre-Pruning:\n",
        "\n",
        "1.To prevent overfitting\n",
        "\n",
        "2.To improve generalization on unseen data\n",
        "\n",
        "3.To reduce tree complexity and training time"
      ],
      "metadata": {
        "id": "UdUfDyo6lBDc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjzSaA4rgmcX",
        "outputId": "cc6d8e3a-1bc8-4b0a-969e-ff57fac9af84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ],
      "source": [
        "# Ques 4>Write a Python program to train a Decision Tree Classifier using Gini\n",
        "#Impurity as the criterion and print the feature importances (practical).\n",
        "# Import required libraries\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data        # Features\n",
        "y = data.target      # Target labels\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create Decision Tree Classifier using Gini Impurity\n",
        "dt = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(data.feature_names, dt.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 5>What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Ans>A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression problems.\n",
        "It works by finding the optimal hyperplane that best separates data points of different classes."
      ],
      "metadata": {
        "id": "BQplegnvmxYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 6>What is the Kernel Trick in SVM?\n",
        "\n",
        "Ans>The Kernel Trick is a technique used in SVM to handle non-linearly separable data by implicitly mapping data into a higher-dimensional feature space, where a linear separator can be found.\n",
        "\n",
        "How the Kernel Trick Works:\n",
        "\n",
        "!.Instead of computing new features, SVM uses a kernel function\n",
        "\n",
        "2.The kernel computes the inner product of data points in a higher-dimensional space\n",
        "\n",
        "3.This allows SVM to find a linear hyperplane in higher dimensions, which corresponds to a non-linear boundary in original space"
      ],
      "metadata": {
        "id": "dZKvXVoUnNxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ques 7>Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "#kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Calculate accuracies\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy using Linear Kernel:\", accuracy_linear)\n",
        "print(\"Accuracy using RBF Kernel:\", accuracy_rbf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYGwpXhUoYCw",
        "outputId": "ad2df9b0-180b-4f57-e4ef-5ff4f0d64ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using Linear Kernel: 0.9814814814814815\n",
            "Accuracy using RBF Kernel: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 8>: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "Ans>The Naïve Bayes classifier is a supervised machine learning algorithm based on Bayes’ Theorem.\n",
        "It is mainly used for classification problems, especially in text classification and spam detection.\n",
        "\n",
        "P(C∣X)=P(X∣C)P(C)/p(x)\n",
        "\n",
        "P(C∣X) = Posterior probability of class\n",
        "\n",
        "C given features X\n",
        "\n",
        "P(X∣C) = Likelihood\n",
        "\n",
        "\n",
        "P(C) = Prior probability of class\n",
        "\n",
        "\n",
        "P(X) = Evidence\n",
        "\n",
        "\n",
        "Why It Is Called “Naïve”?\n",
        "\n",
        "The classifier is called naïve because it makes a strong assumption:\n",
        "\n",
        " All features are conditionally independent of each other given the class label.\n",
        "\n",
        "This assumption is often not true in real-world data, but the algorithm still performs surprisingly well."
      ],
      "metadata": {
        "id": "YTDmuY7eqD0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 9>Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "\n",
        "Ans> 1. Gaussian Naïve Bayes\n",
        "\n",
        "Used when features are continuous and follow a normal (Gaussian) distribution\n",
        "\n",
        "Assumes data is distributed according to a bell-shaped curve\n",
        "\n",
        "Commonly used in medical data, sensor data, and numerical datasets\n",
        "\n",
        "Example: Height, weight, temperature\n",
        "\n",
        "2. Multinomial Naïve Bayes\n",
        "\n",
        "Used for discrete count-based features\n",
        "\n",
        "Commonly applied in text classification problems\n",
        "\n",
        "Works well with word frequencies or term counts\n",
        "\n",
        "Example: Number of times a word appears in a document\n",
        "\n",
        "3. Bernoulli Naïve Bayes\n",
        "\n",
        "Used for binary features (0 or 1)\n",
        "\n",
        "Focuses on whether a feature is present or absent\n",
        "\n",
        "Suitable for binary text features\n",
        "\n",
        "Example: Word appears or does not appear in a document\n",
        "\n",
        "\n",
        "| Aspect              | Gaussian NB               | Multinomial NB         | Bernoulli NB               |\n",
        "| ------------------- | ------------------------- | ---------------------- | -------------------------- |\n",
        "| Type of Features    | Continuous                | Discrete counts        | Binary                     |\n",
        "| Data Distribution   | Gaussian (Normal)         | Multinomial            | Bernoulli                  |\n",
        "| Typical Use Case    | Numerical datasets        | Text classification    | Binary text classification |\n",
        "| Feature Values      | Real numbers              | Integers (0, 1, 2, …)  | 0 or 1                     |\n",
        "| Sensitivity         | Sensitive to distribution | Sensitive to frequency | Sensitive to presence      |\n",
        "| Example Application | Medical diagnosis         | Spam detection         | Sentiment analysis         |\n"
      ],
      "metadata": {
        "id": "C0znSry8r36f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ques 10>Question 10: Breast Cancer Dataset\n",
        "#Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "#dataset and evaluate accuracy.\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data      # Features\n",
        "y = data.target    # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Accuracy of Gaussian Naïve Bayes Classifier:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkOQSHMDs0ZK",
        "outputId": "fe2b122f-fa1f-4654-f074-c2f28b725953"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes Classifier: 0.9415204678362573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ktdGDxsos0UH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}